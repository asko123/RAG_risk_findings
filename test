# test/test_scanner_handler_aws_operator.py
# -----------------------------------------------------------------------------
# Purpose
# -------
# End-to-end moto-based test for scanner_handler.init_bucket_scan using
# a compliant S3 bucket and a "no policy" bucket.
# This version includes a canonicalizer for the VPCE condition to match
# what the scanner expects and avoids common pitfalls (SourceVpc vs SourceVpce,
# StringEquals vs StringNotEquals, scalar vs list).
#
# Assumptions (explicit):
# - Module paths: `common.aws_operator.AWSOperator` and `scanner.scanner_handler`
#   exist in your repo.
# - Reference policy JSON lives at
#   "test/data/bucket_security/generate_ref_policy_Pass1.json" and contains
#   a *list of statements* (not a full policy object).
# - Event JSONs are placed at "test/data/scanner_handler/".
# -----------------------------------------------------------------------------

import os                # stdlib: env var for selecting 'lab' mode
import json              # stdlib: JSON (load/dump)
import unittest          # stdlib: unit test framework

import boto3             # AWS SDK used with moto's in-memory backend
from moto import mock_aws  # moto v5+ consolidated decorator/context

from common.aws_operator import AWSOperator        # SUT dependency
from scanner import scanner_handler                # function under test


# Force code-under-test to take 'lab' branch (matches your runtime screenshots)
os.environ["env"] = "lab"


def _as_bool(v):
    """Normalize 'true'/'false'/bool to a Python bool for robust assertions."""
    if isinstance(v, bool):
        return v
    return str(v).strip().lower() in ("true", "1", "yes")


def _read_json(rel_path):
    """Read a JSON file relative to repo root for test-data convenience."""
    # Try the path as given; if that fails, try relative to this file
    candidates = [
        rel_path,
        os.path.join(os.path.dirname(__file__), rel_path.replace("test/", "")),
        os.path.join(os.path.dirname(__file__), rel_path),
    ]
    for p in candidates:
        try:
            with open(p, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            continue
    raise FileNotFoundError(f"Could not locate {rel_path} in {candidates}")


def _fix_vpce_condition(statements, vpce_id):
    """
    Normalize 'DenyUnlessVPCEndpoint' style statements so the scanner sees:
      Condition: { "StringNotEquals": { "aws:SourceVpce": [vpce_id] } }
    Also repairs 'aws:SourceVpc' -> 'aws:SourceVpce' and ensures list typing.
    """
    for st in statements:
        # Get SID & condition (be tolerant of missing fields)
        sid = (st.get("Sid") or "").lower()
        cond = st.get("Condition", {})

        # 1) Rename SourceVpc -> SourceVpce anywhere it appears
        for op, block in list(cond.items()):
            if isinstance(block, dict) and "aws:SourceVpc" in block:
                # move to the expected key
                block["aws:SourceVpce"] = block.pop("aws:SourceVpc")
                # enforce list form
                if not isinstance(block["aws:SourceVpce"], list):
                    block["aws:SourceVpce"] = [block["aws:SourceVpce"]]
                cond[op] = block

        # 2) If this looks like the VPCE guardrail SID, force canonical form
        if "vpce" in sid or "denyunlessvpce" in sid or "denyunlessvpcendpoint" in sid:
            cond = {"StringNotEquals": {"aws:SourceVpce": [vpce_id]}}

        st["Condition"] = cond
    return statements


class TestScannerHandlerAWSOperator(unittest.TestCase):
    """End-to-end scanner tests with moto-backed AWS services."""

    def setUp(self):
        # Start moto (fresh in-memory AWS for each test)
        self._moto = mock_aws()
        self._moto.start()

        # Region for all clients
        self.region = "us-east-1"

        # AWS service clients (moto-backed)
        self.s3  = boto3.client("s3",  region_name=self.region)
        self.sns = boto3.client("sns", region_name=self.region)
        self.iam = boto3.client("iam", region_name=self.region)
        self.ec2 = boto3.client("ec2", region_name=self.region)

        # ---------------------------
        # 1) Buckets under test
        # ---------------------------
        # Compliant bucket
        self.s3.create_bucket(Bucket="dfs-lab16-config-bucket")
        # Bare bucket (used in the "no policy" test)
        self.s3.create_bucket(Bucket="blank_policy")

        # Tags required by your handler (FIX: use 'secmon_id', not 'secom_id')
        self.s3.put_bucket_tagging(
            Bucket="dfs-lab16-config-bucket",
            Tagging={
                "TagSet": [
                    {"Key": "secmon_id", "Value": "400"},
                    {"Key": "deployer_role", "Value": "LAB16"},
                ]
            },
        )

        # Versioning enabled
        self.s3.put_bucket_versioning(
            Bucket="dfs-lab16-config-bucket",
            VersioningConfiguration={"Status": "Enabled"},
        )

        # Default bucket encryption (AES256)
        self.s3.put_bucket_encryption(
            Bucket="dfs-lab16-config-bucket",
            ServerSideEncryptionConfiguration={
                "Rules": [
                    {"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}
                ]
            },
        )

        # Optionally ensure ACL is private (some scanners verify this)
        self.s3.put_bucket_acl(Bucket="dfs-lab16-config-bucket", ACL="private")

        # ---------------------------
        # 2) Build VPC endpoint and patch reference policy with its ID
        # ---------------------------
        # Create a VPC (moto returns randomized IDs)
        vpc_id = self.ec2.create_vpc(CidrBlock="10.0.0.0/16")["Vpc"]["VpcId"]
        # Create a Gateway VPC endpoint for S3 (us-east-1)
        vpce_id = self.ec2.create_vpc_endpoint(
            VpcId=vpc_id, ServiceName="com.amazonaws.us-east-1.s3", VpcEndpointType="Gateway"
        )["VpcEndpoint"]["VpcEndpointId"]

        # Load your curated "pass" statements and normalize VPCE condition
        statements = _read_json("test/data/bucket_security/generate_ref_policy_Pass1.json")
        statements = _fix_vpce_condition(statements, vpce_id)

        # Compose and set the bucket policy
        compliant_policy = {"Version": "2012-10-17", "Statement": statements}
        self.s3.put_bucket_policy(
            Bucket="dfs-lab16-config-bucket",
            Policy=json.dumps(compliant_policy),
        )

        # ---------------------------
        # 3) SNS topic for notifications
        # ---------------------------
        self.topic_arn = self.sns.create_topic(Name="warden-notify")["TopicArn"]

        # ---------------------------
        # 4) Create IAM roles referenced by logs (not required, keeps logs clean)
        # ---------------------------
        for role in (
            "LAB16",
            "SecurityMonitoringRole",
            "VAULT-ACCT-OWNER",
            "SAML-ACCT-OWNER",
            "SAML-ASSUMADMIN",
            "PROTECTED-TAGATRON-TAG-EXECUTION-SERVICE-LAMBDA-ROLE",
        ):
            try:
                self.iam.create_role(RoleName=role, AssumeRolePolicyDocument="{}")
            except self.iam.exceptions.EntityAlreadyExistsException:
                pass

        # ---------------------------
        # 5) Patch AWSOperator.publish_to_sns to always use our moto topic
        # ---------------------------
        self._orig_publish = AWSOperator.publish_to_sns

        def _publish_to_sns(_self, message):
            boto3.client("sns", region_name=self.region).publish(
                TopicArn=self.topic_arn, Message=json.dumps(message)
            )

        AWSOperator.publish_to_sns = _publish_to_sns

        # Create the operator (SUT dependency)
        self.operator = AWSOperator()

    def tearDown(self):
        # Restore original method and stop moto
        AWSOperator.publish_to_sns = self._orig_publish
        self._moto.stop()

    # ---------------------------
    # Helpers
    # ---------------------------
    def _event(self, filename):
        return _read_json(f"test/data/scanner_handler/{filename}")

    def _run_and_assert(self, event_file, expected_compliant_bool):
        # Load CloudTrail-like event
        event = self._event(event_file)
        bucket = event["detail"]["bucketName"]

        # For the "withoutPolicy" test, delete the policy of the target bucket
        if event_file == "event_ScanBucket_withoutPolicy.json":
            try:
                self.s3.delete_bucket_policy(Bucket=bucket)
            except Exception:
                pass

        # Call the code under test
        resp = scanner_handler.init_bucket_scan(event, operator=self.operator)

        # Sanity: ensure we scanned the intended bucket
        self.assertIn("bucket_name", resp)
        self.assertEqual(resp["bucket_name"], bucket)

        # Normalize 'compliant' to bool and assert
        actual = _as_bool(resp.get("compliant", False))
        self.assertEqual(actual, expected_compliant_bool, msg=json.dumps(resp, indent=2))

    # ---------------------------
    # Tests
    # ---------------------------
    def test_compliant_bucket(self):
        # Event must target the configured/secured bucket
        self._run_and_assert("event_ScanBucket_compliant.json", True)

    def test_bucket_without_policy(self):
        # Event must target the blank bucket
        self._run_and_assert("event_ScanBucket_withoutPolicy.json", False)


if __name__ == "__main__":
    unittest.main(verbosity=2)
