import os
import pandas as pd
import pdfplumber
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline
from sklearn.metrics.pairwise import cosine_similarity

# Load the tokenizer and model from Hugging Face
tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Load the SentenceTransformer model for embedding generation
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize FAISS index
dimension = 384  # Dimension of embeddings generated by 'all-MiniLM-L6-v2'
index = faiss.IndexFlatL2(dimension)
document_store = []  # To keep track of document content
embeddings_store = []  # To keep track of document embeddings

# Function to read CSV and PDF files
def read_document(file_path):
    extension = os.path.splitext(file_path)[1].lower()
    text_data = []
    
    if extension == '.csv':
        try:
            df = pd.read_csv(file_path)
            text = df.astype(str).agg(' '.join, axis=1).tolist()
            text_data.extend(text)
        except Exception as e:
            text_data.append(f"Error reading CSV file: {str(e)}")
    elif extension == '.pdf':
        try:
            text = ''
            tables = []
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + '\n'
                    
                    page_tables = page.extract_tables()
                    for table in page_tables:
                        tables.append(table)

            for table in tables:
                table_text = '\n'.join([', '.join(filter(None, row)) for row in table if any(row)])
                text += f"\nTable:\n{table_text}\nEND TABLE\n"
            text_data.append(text)
        except Exception as e:
            text_data.append(f"Error reading PDF file: {str(e)}")
    else:
        text_data.append(f"Unsupported file type: {extension}")
    
    return ' '.join(text_data)

# Prompt template as a cybersecurity expert
def cyber_security_prompt(document_content):
    system_message = "You are a helpful AI assistant for cybersecurity, specializing in standards, policies, and remediation strategies."
    prompt_template = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_message}<|eot_id|>\n"
        "<|start_header_id|>user<|end_header_id|>\n"
        "You are a cybersecurity expert. Analyze the following data and provide insights on potential risks, recommendations for security standards, and steps to remediate issues. Your response should be tailored to developers or security professionals seeking to implement best practices.\n"
        "Document Content:\n" + document_content + "<|eot_id|>\n"
        "<|start_header_id|>assistant<|end_header_id|>"
    )
    return prompt_template

# Main function to demonstrate RAG using Llama-3.1-8B-Instruct and FAISS with reranking
def rag_demo(data_folder):
    # Iterate through all files in the data folder
    for file_name in os.listdir(data_folder):
        file_path = os.path.join(data_folder, file_name)
        
        # Extract content from the document
        document_content = read_document(file_path)
        
        # Generate embeddings for the document and add to FAISS index
        embedding = embedding_model.encode(document_content)
        index.add(np.array([embedding]))
        document_store.append(document_content)
        embeddings_store.append(embedding)
    
    # Example query to demonstrate retrieval
    query = "Analyze the document for cybersecurity vulnerabilities."
    query_embedding = embedding_model.encode(query)
    
    # Retrieve the nearest documents
    _, nearest_indices = index.search(np.array([query_embedding]), k=5)
    nearest_documents = [document_store[i] for i in nearest_indices[0]]
    nearest_embeddings = [embeddings_store[i] for i in nearest_indices[0]]
    
    # Rerank the retrieved documents using cosine similarity
    similarities = cosine_similarity([query_embedding], nearest_embeddings)[0]
    reranked_indices = np.argsort(similarities)[::-1]
    best_document_content = nearest_documents[reranked_indices[0]]
    
    # Create the prompt and generate the response
    prompt = cyber_security_prompt(best_document_content)
    response = generator(prompt, max_length=500, num_return_sequences=1)[0]['generated_text']
    
    # Print the generated response
    print(f"Response for the retrieved document:\n{response}\n")

# Example usage
if __name__ == "__main__":
    data_folder = "data"  # Replace with your data folder path containing CSV and PDF files
    rag_demo(data_folder)
