import os
import pandas as pd
import pdfplumber
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from transformers import LlamaForCausalLM, LlamaTokenizer, pipeline
from sklearn.metrics.pairwise import cosine_similarity

# Load the tokenizer and model from Hugging Face
tokenizer = LlamaTokenizer.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
model = LlamaForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# Load the SentenceTransformer model for embedding generation
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Initialize FAISS index
dimension = 384  # Dimension of embeddings generated by 'all-MiniLM-L6-v2'
index = faiss.IndexFlatL2(dimension)
document_store = []  # To keep track of document content
embeddings_store = []  # To keep track of document embeddings

# Function to read CSV and PDF files
def read_document(file_path):
    extension = os.path.splitext(file_path)[1].lower()
    text_data = []
    
    if extension == '.csv':
        try:
            df = pd.read_csv(file_path)
            text = df.astype(str).agg(' '.join, axis=1).tolist()
            text_data.extend(text)
        except Exception as e:
            text_data.append(f"Error reading CSV file: {str(e)}")
    elif extension == '.pdf':
        try:
            text = ''
            tables = []
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + '\n'
                    
                    page_tables = page.extract_tables()
                    for table in page_tables:
                        tables.append(table)

            for table in tables:
                table_text = '\n'.join([', '.join(filter(None, row)) for row in table if any(row)])
                text += f"\nTable:\n{table_text}\nEND TABLE\n"
            text_data.append(text)
        except Exception as e:
            text_data.append(f"Error reading PDF file: {str(e)}")
    else:
        text_data.append(f"Unsupported file type: {extension}")
    
    return ' '.join(text_data)

# Prompt template for addressing specific findings
def cyber_security_prompt(finding, document_content):
    system_message = "You are a helpful AI assistant for cybersecurity, specializing in standards, policies, and remediation strategies."
    prompt_template = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_message}<|eot_id|>\n"
        "<|start_header_id|>user<|end_header_id|>\n"
        "You are a cybersecurity expert. Given the following finding, curate a response using the provided documents to address the issue, recommend best practices, and suggest remediation steps.\n"
        "Finding:\n" + finding + "\n"
        "Document Content:\n" + document_content + "<|eot_id|>\n"
        "<|start_header_id|>assistant<|end_header_id|>"
    )
    return prompt_template

# Main function to demonstrate RAG using Llama-3.1-8B-Instruct and FAISS with reranking
def rag_demo(data_folder):
    # Iterate through all files in the data folder
    for file_name in os.listdir(data_folder):
        file_path = os.path.join(data_folder, file_name)
        
        # Extract content from the document
        document_content = read_document(file_path)
        
        # Generate embeddings for the document and add to FAISS index
        embedding = embedding_model.encode(document_content)
        index.add(np.array([embedding]))
        document_store.append(document_content)
        embeddings_store.append(embedding)
    
    # Loop to allow users to ask multiple related findings questions
    while True:
        finding = input("Enter the finding (or type 'exit' to quit): ")
        if finding.lower() == 'exit':
            break
        
        # Generate embedding for the query
        query_embedding = embedding_model.encode(finding)
        
        # Retrieve the nearest documents
        # Search the FAISS index to find the top k=5 nearest document embeddings to the query
        _, nearest_indices = index.search(np.array([query_embedding]), k=5)
        
        # Extract the content and embeddings of the nearest documents
        nearest_documents = [document_store[i] for i in nearest_indices[0]]
        nearest_embeddings = [embeddings_store[i] for i in nearest_indices[0]]
        
        # Rerank the retrieved documents using cosine similarity
        # Calculate the cosine similarity between the query embedding and each of the retrieved document embeddings
        similarities = cosine_similarity([query_embedding], nearest_embeddings)[0]
        
        # Sort the documents by similarity in descending order to get the most relevant document first
        reranked_indices = np.argsort(similarities)[::-1]
        
        # Select the best document based on the highest similarity score
        best_document_content = nearest_documents[reranked_indices[0]]
        
        # Create the prompt and generate the response
        prompt = cyber_security_prompt(finding, best_document_content)
        response = generator(prompt, max_length=500, num_return_sequences=1)[0]['generated_text']
        
        # Print the generated response
        print(f"Response for the finding:\n{response}\n")

# Example usage
if __name__ == "__main__":
    data_folder = "data"  # Replace with your data folder path containing CSV and PDF files
    rag_demo(data_folder)
